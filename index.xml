<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math Lacome on Math Lacome</title>
    <link>/</link>
    <description>Recent content in Math Lacome on Math Lacome</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Modeling intensity vs time.</title>
      <link>/post/model-peak-intensity/</link>
      <pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/model-peak-intensity/</guid>
      <description>&lt;p&gt;One of the challenges to assess match demands is that the intensity and density of actions is likely time-independent, i.e., the longer the period, the lower the average intensity. &lt;a href=&#34;https://www.researchgate.net/publication/315750153_Modelling_the_decrement_in_running_intensity_within_professional_soccer_players&#34;&gt;Delaney et al.&lt;/a&gt; proposed to model match-related locomotor intensity vs. time relationship during matches using a power relationship. We used this method in a &lt;a href=&#34;http://mathlacome.rbind.io/publication/2017-ssg-football/&#34;&gt;recent paper&lt;/a&gt; to examine at which extent different SSG formats could be used to either under- or overload the running- and/or mechanical- demands of competitive matches. In this post, we will see how to perform these analyses in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)       
library(&amp;quot;readxl&amp;quot;)
library(RcppRoll)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, I need to import csv files to create my database. To work around these models, we usually use multiple csv. An easy and efficient way to import a batch of files is to &lt;em&gt;map&lt;/em&gt; the &lt;em&gt;read_csv&lt;/em&gt; function over the differents files.&lt;/p&gt;
&lt;p&gt;For this, the first step is to list all the files into our directory and then path the list of files as an argument to the function read_csv with the map function from the purrr package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_files &amp;lt;- list.files(path = &amp;quot;C:/Users/mlacome/Documents/MathLacome-2018-blog/1_DataSets/0_GameData&amp;quot;, pattern=&amp;quot;*.csv&amp;quot;, full.names = TRUE)
my_file &amp;lt;- map_dfr(my_files, read_csv, skip = 2, .id = &amp;quot;FileName&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   `PLAYER NAME` = col_character(),
##   `TEAM FIXTURE` = col_character(),
##   `MATCH FIXTURE` = col_character(),
##   HALF = col_character(),
##   TIME = col_double(),
##   `Player X Position` = col_integer(),
##   `Player Y Position` = col_integer()
## )
## Parsed with column specification:
## cols(
##   `PLAYER NAME` = col_character(),
##   `TEAM FIXTURE` = col_character(),
##   `MATCH FIXTURE` = col_character(),
##   HALF = col_character(),
##   TIME = col_double(),
##   `Player X Position` = col_integer(),
##   `Player Y Position` = col_integer()
## )
## Parsed with column specification:
## cols(
##   `PLAYER NAME` = col_character(),
##   `TEAM FIXTURE` = col_character(),
##   `MATCH FIXTURE` = col_character(),
##   HALF = col_character(),
##   TIME = col_double(),
##   `Player X Position` = col_integer(),
##   `Player Y Position` = col_integer()
## )
## Parsed with column specification:
## cols(
##   `PLAYER NAME` = col_character(),
##   `TEAM FIXTURE` = col_character(),
##   `MATCH FIXTURE` = col_character(),
##   HALF = col_character(),
##   TIME = col_double(),
##   `Player X Position` = col_integer(),
##   `Player Y Position` = col_integer()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(my_file)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##   FileName `PLAYER NAME` `TEAM FIXTURE` `MATCH FIXTURE` HALF        TIME
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 1        Player1       Team1          Team2 v Team1   First Half 0.   
## 2 1        Player1       Team1          Team2 v Team1   First Half 0.100
## 3 1        Player1       Team1          Team2 v Team1   First Half 0.200
## 4 1        Player1       Team1          Team2 v Team1   First Half 0.300
## 5 1        Player1       Team1          Team2 v Team1   First Half 0.400
## 6 1        Player1       Team1          Team2 v Team1   First Half 0.500
## # ... with 2 more variables: `Player X Position` &amp;lt;int&amp;gt;, `Player Y
## #   Position` &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We imported tracking data with only Players x &amp;amp; y positions. Using mutate, we can calculate distances, speeds &amp;amp; thus, distances covered in different speed zones.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Game_analysis &amp;lt;- my_file %&amp;gt;%
  group_by(`PLAYER NAME`, `MATCH FIXTURE`, HALF) %&amp;gt;%
  mutate(Distance_m = (sqrt((`Player X Position` - lag(`Player X Position`))^2 + (`Player Y Position` - lag(`Player Y Position`))^2)) / 100) %&amp;gt;%
  mutate(Speed_km.h = (Distance_m / 0.1)*3.6) %&amp;gt;%
  mutate(Distance_14.4 = ifelse(Speed_km.h &amp;gt; 14.4, Distance_m, NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Delaney used different rolling average (from 1 to 15-min) to do his model. We can set the duration using a parameter. As my files have a frequency of 10Hz, I only need to multiply the number of minutes by 600.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roll_mean_dur &amp;lt;- c(1:15)*600L&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to perform the rolling average on distance covered &amp;amp; distance above 14.4 km.h-1 per minute in this exemple. The first step is to create a function (rolling_foo) that will store the rolling sum of distance covered. Then, we will use the map_dfr to apply our function for each rolling duration and store the results in a dataframe. We need approximatively 1min to analyse 4 files of 90+ minutes. I’m sure there is possibility to get something faster and would be happy to get your feedbacks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;start_seg &amp;lt;- Sys.time()

rolling_foo &amp;lt;- function(x){
    res &amp;lt;- Game_analysis %&amp;gt;%
    group_by(`PLAYER NAME`, `MATCH FIXTURE`, HALF) %&amp;gt;%
    mutate(Roll_dur = x) %&amp;gt;%
    mutate(Distance = roll_sum(Distance_m, x, by=1L, fill=NA, na.rm=T)) %&amp;gt;%
    mutate(DistanceHS = roll_sum(Distance_14.4, x, by=1L, fill=NA, na.rm=T))
}

Game_analysis_2 &amp;lt;- map_dfr(roll_mean_dur, rolling_foo) %&amp;gt;%
  group_by(`PLAYER NAME`, `MATCH FIXTURE`, Roll_dur) %&amp;gt;%
  summarise(Peak_Dist_min = max(Distance, na.rm=T)/(mean(Roll_dur, na.rm=T)/600),
            Peak_DistHS_min = max(DistanceHS, na.rm=T)/(mean(Roll_dur, na.rm=T)/600))

end_seg &amp;lt;- Sys.time()
(end_seg - start_seg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 58.9198 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As described in Delaney paper, a power law curve describes non-linear but clearly dependent relationships between two variables (x and y) &amp;amp; can be given by the equation: y = c * x^n where n and c are constants. To conclude this post, we need to perform the linear relationships on the log of Distances &amp;amp; log of durations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(Peak_dist_model &amp;lt;- lm(log(Peak_Dist_min) ~ log(Roll_dur/600), data = Game_analysis_2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = log(Peak_Dist_min) ~ log(Roll_dur/600), data = Game_analysis_2)
## 
## Coefficients:
##       (Intercept)  log(Roll_dur/600)  
##            5.3120            -0.1626&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(Peak_Dist_Intercept &amp;lt;- exp(Peak_dist_model$coefficients[1]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##    202.7516&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(Peak_Dist_Slope &amp;lt;- Peak_dist_model$coefficients[2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## log(Roll_dur/600) 
##        -0.1626441&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Visualising force load distribution over time</title>
      <link>/post/sparkline-disbalance/</link>
      <pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/sparkline-disbalance/</guid>
      <description>&lt;p&gt;Recently, interest for GPS-embedded accelerometers variables has grown. Indeed, while classic locomotors variables give many pieces of information about the external load, little is known about how players’ cope with this load in return. GPS-embedded accelerometers, in return, allow the calculation of promising variables and among them, Force load. Force load refers to the sum of estimated ground reaction forces during all foot impacts, assessed via the accelerometer-derived magnitude vector ( &lt;a href=&#34;http://mathlacome.rbind.io/publication/2018-aspetar-rome1/&#34;&gt;Lacome et al.&lt;/a&gt;). Force load can be compared between left and right foot and be analysed to identify any potential weaknesses of a muscle group. Despite that one number in isolation means nothing when analysing potential imbalances - some players being not ‘balanced’ due to specific adaptation - looking at changes in force load distribution can potentially reveal some great insights.&lt;/p&gt;
&lt;p&gt;In this post, we will see how to create a simple dashboard to track changes in force load distribution over time. We will create a simple sparkline to get an overview of the data and then a bubble chart to deep into specific variables.&lt;/p&gt;
&lt;p&gt;I guess you know how to import data into Tableau now. if not, check my &lt;a href=&#34;http://mathlacome.rbind.io/post/performance-comparison/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that I have imported my ‘force load distribution’ database, I can start working. As I have some NULL value, Tableau specify the type of data as “STRING”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I change the class of my variable from “STRING” to “NUMBERS”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-2.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Place ‘Measure Names’, then ‘Measure Values’ on your ‘Rows’ shelf.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-3.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Place a date field on the ‘Columns’ shelf. If you are following on your own Tableau software, you can see that Tableau aggregate the Date fields by YEARS. By right clicking on it, I changed the aggregation to “WEEKS”. Then, you can see that Tableau aggregate by default my ‘Measure Values’ (see bottom-left of the caption) as a Sum. While summing force load distribution by week is nonsense, averaging is much better. Right-click on the pills and we can change the aggregation to average.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-4.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sparklines are generally used to provide an idea of the trend at a glance. So, usually, we remove the y-axis labels. As we are interested also about the side of the potential disbalance, I like using a 0-centred sparkline. To do this, right click on the y-axis and modify it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-5.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For the purpose of my dashboard, I change the ‘Measure Name’ from the ‘Rows’ shelf to the ‘Column’ one.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-6.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sparklines are done. It’s only time for some formatting stuff and everyone is free to do it his way! Let’s now move onto part 2 and create the bubble chart. First of all, I only want 1 bubble chart in my Dashboard. The idea is that the user looks at the sparkline, see a trend and look at the bubble chart for more information on one specific variable.&lt;/p&gt;
&lt;p&gt;The first part is then to create a parameter to allow the end-user to choose the variable he wants to see.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-7.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have created a parameter which allows the selection of 1 out of 4 values. From this parameter, we can create another calculated fields that will say “If you have selected A, give me Variable A (…)” The best option for this is to use a CASE statement.&lt;/p&gt;
&lt;p&gt;Case statement said: CASE “Variable” (in our case the parameter) WHEN A then A1 WHEN B then B1 (…)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-8.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next step - we can drag the newly created variable (here Variable_value) into the ‘Rows’ shelf and the date one into the ‘column’ shelf. For this bubble chart, I don’t want ‘date’ to be aggregated, so I select ‘Days’ and set them as a continuous variable. On the ‘Mark’ shelf, I change Lines for Circles.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-9.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My next step is again to simplify the end-user job by providing some colours when values are abnormally high or low. We will create another calculated field as below…&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-10&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/3_Disbalances/Dis-10.png&#34; alt=&#34;Case statement&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Case statement
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;… and drag this new dimension into the ‘Color’ mark.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-11.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I like using bands to figure out the ‘normal range’. We can go to ‘Analytics’ and drag a reference band.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/3_Disbalances/Dis-12.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our bubble chart is now ready. We can start merging everything into a nice dashboard and make it engaging for your end-user. If you want to play around with my dashboard and send me some comments, I’ll be happy to improve this one. Let me know if you think that clarity of the information displayed or end-user overall experience can be improved. In my next post about Tableau, I will try to go a bit deeper. Most of the time, these analyses need to be individualised and we will see how can we adapt the band based on potential SWC or effect sizes.&lt;/p&gt;
&lt;iframe align = &#34;center&#34; width = &#34;700&#34; height = &#34;550&#34; src=&#34;https://public.tableau.com/profile/mathlacome#!/vizhome/Disbalances-Analysis/Tableaudebord1??:showVizHome=no&amp;:embed=true&#34;/&gt;
</description>
    </item>
    
    <item>
      <title>Using external load to estimate HR responses.</title>
      <link>/post/hr-prediction/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/hr-prediction/</guid>
      <description>&lt;p&gt;In my last post, we saw a method to perform simple between-players normalisation to get first insights on players’ “performance” status. Like every model, this one has some known limitations and thus, it is important to use it in combination with other models to create a full stream of information. To overcome the limitations inherent to the use of this method, examining the dose-response relationship between workload and immediate physiological responses (or more simply generic models of work efficiency, i.e., output/cost relationships) may represent an advance to assess training status from data collected routinely in elite players.&lt;/p&gt;
&lt;p&gt;We recently published &lt;a href=&#34;http://mathlacome.rbind.io/publication/2018-readiness-predhr/&#34;&gt;a paper&lt;/a&gt; that examines the ability of multivariate models to predict the HR responses to some specific training drills from various GPS variables and reports the usefulness of the difference in predicted vs actual HR responses as an index of fitness or readiness to perform. We find that these models had the ability to detect when the players were fit vs not fit and can be a tool to replace tedious &amp;amp; time-consuming formal testing.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/2_HRpred/Fig2.png&#34; alt=&#34;Changes in Mechanical Work (a.u, upper panel), HRd and HRRUN (lower panel) during pre-season and early in season in one representative elite soccer player.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Changes in Mechanical Work (a.u, upper panel), HRd and HRRUN (lower panel) during pre-season and early in season in one representative elite soccer player.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For more information about this model and how we have tried to validate it, please &lt;a href=&#34;http://mathlacome.rbind.io/publication/2018-readiness-predhr/&#34;&gt;clic here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s see how to do it in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)       
library(&amp;quot;readxl&amp;quot;)
library(&amp;quot;tidyverse&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As always, we start by importing our database into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Master &amp;lt;- readRDS(&amp;quot;~/MathLacome-2018-blog/content/post/Master.rds&amp;quot;)
head(Master,10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 10
##    Surname    Date `Drill Category` `%Hrmax` Av.spd `vL/min` `fL/min`
##    &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 P2      140630. Session              72.7   96.8     6.81     9.29
##  2 P2      140630. WU                   70.4  150.     11.8     14.0 
##  3 P2      140630. WU                   70.1  102.      8.00     9.15
##  4 P2      140630. Ph                   80.6  102.      7.53    10.2 
##  5 P2      140630. Po                   76.2   80.8     5.02     8.82
##  6 P3      140630. Session              NA     95.1     5.85     6.81
##  7 P3      140630. WU                   NA    151.     10.1     11.1 
##  8 P3      140630. WU                   NA    102.      7.28     6.87
##  9 P3      140630. Ph                   NA    105.      6.93     7.39
## 10 P3      140630. Po                   NA     73.9     3.83     5.76
## # ... with 3 more variables: `V1/min` &amp;lt;dbl&amp;gt;, `V2/min` &amp;lt;dbl&amp;gt;, `MECH
## #   WORK/min` &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now proceeding to some data cleaning.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# DATA.FRAME PREP - CLEANING -----------------------------------
Master &amp;lt;- Master %&amp;gt;%
  #Some of my value are not store as numeric - I change the case for these ones.
  mutate(MechWmin = as.numeric(as.character(`MECH WORK/min`))) %&amp;gt;%
  mutate(vL.min = `vL/min`) %&amp;gt;%
  mutate(fL.min = `fL/min`) %&amp;gt;%
  mutate(V1.min = `V1/min`) %&amp;gt;%
  mutate(V2.min = `V2/min`) %&amp;gt;%
  dplyr::select(-c(`MECH WORK/min`, `vL/min`, `fL/min`, `V1/min`, `V2/min`)) %&amp;gt;%
  # I change the surname as factors.
  mutate(Surname = as.factor(Surname))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;hr-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HR models&lt;/h2&gt;
&lt;p&gt;In this part of the code, we will calculate the model presented in our paper published in &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/29688115&#34;&gt;IJSPP&lt;/a&gt;. For this, we need to keep only the column we are interesting in. Within-player models will then be created using the step::stats function that allow to choose a model by AIC in a stepwise algorithm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bdd &amp;lt;- Master %&amp;gt;%
  select(-Date) %&amp;gt;%
  filter(`Drill Category` == &amp;quot;Po&amp;quot; | `Drill Category` == &amp;quot;GS&amp;quot;) %&amp;gt;%
  select(-`Drill Category`)

# We create a Variable that will store our results &amp;quot;Res&amp;quot;
res&amp;lt;-c()

# We create a function that will perform a Step multiple regression (lm) on each player (Surname)
listeRes2 &amp;lt;- lapply(X = unique(bdd$Surname), FUN = function(p){
  #tryCatch function allow to store error &amp;amp; avoid function to crash.
  out &amp;lt;- tryCatch(
    {
      bddjoueur &amp;lt;- bdd %&amp;gt;%
  # First step is to filter Surname = p, than keep only HR &amp;amp; GPS data in the data.frame by dropping Surname variable..
        filter(Surname == p) %&amp;gt;%
        select(-Surname)
    
  # Second step, performing the step model on each subset.   
      model &amp;lt;- step(object = lm(`%Hrmax`~., data= na.omit(bddjoueur)),direction = &amp;quot;both&amp;quot;)
      print(p)
 # For some players, I have only a few points (academy players for exemple) - I will remove them later if needed.
      Nb_obs &amp;lt;- nrow(bddjoueur)
# We store the model coefficients &amp;amp; r.squared in the variable &amp;quot;res&amp;quot; to use it later.      
      res&amp;lt;-t(as.data.frame(model$coefficients)) %&amp;gt;%
        cbind(as.data.frame(summary(model)$adj.r.squared)) %&amp;gt;%
        mutate(Surname = p) %&amp;gt;%
# As we use a step model, we don&amp;#39;t have the same number of column by players because some are dropped by the step model. The trick is to change the wide format (1 column per variable) to a long format (1 column to store the variable name &amp;amp; 1 for values). We will then be able to bind the data.frame and transform it back in a proper wide format.
        gather(-Surname, key = &amp;quot;variable&amp;quot;, value = &amp;quot;value&amp;quot;)
      
      return(res)
    },
# My model was crashing due to players with only 1 or 2 row of data. One option could have been to count the number of occurence and filter them. Insteap I decide to use a trycatch to store on one side good models &amp;amp; store null values when I have errors.
    error = function(p)
      return(NULL)
  )
  return(out)
}
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Start:  AIC=307.17
## `%Hrmax` ~ Av.spd + MechWmin + vL.min + fL.min + V1.min + V2.min
## 
##            Df Sum of Sq    RSS    AIC
## - V2.min    1      3.58 1209.1 305.57
## - Av.spd    1      8.41 1213.9 306.10
## - fL.min    1     16.70 1222.2 307.00
## &amp;lt;none&amp;gt;                  1205.5 307.17
## - vL.min    1     27.78 1233.3 308.20
## - V1.min    1     42.16 1247.7 309.74
## - MechWmin  1    413.51 1619.0 344.40
## 
## Step:  AIC=305.57
## `%Hrmax` ~ Av.spd + MechWmin + vL.min + fL.min + V1.min
## 
##            Df Sum of Sq    RSS    AIC
## - Av.spd    1      6.22 1215.3 304.25
## - fL.min    1     17.13 1226.2 305.44
## &amp;lt;none&amp;gt;                  1209.1 305.57
## - vL.min    1     24.27 1233.3 306.21
## + V2.min    1      3.58 1205.5 307.17
## - V1.min    1     61.13 1270.2 310.13
## - MechWmin  1    449.23 1658.3 345.59
## 
## Step:  AIC=304.25
## `%Hrmax` ~ MechWmin + vL.min + fL.min + V1.min
## 
##            Df Sum of Sq    RSS    AIC
## &amp;lt;none&amp;gt;                  1215.3 304.25
## - vL.min    1     23.81 1239.1 304.83
## + Av.spd    1      6.22 1209.1 305.57
## + V2.min    1      1.39 1213.9 306.10
## - V1.min    1     55.02 1270.3 308.14
## - fL.min    1     76.49 1291.8 310.37
## - MechWmin  1    475.06 1690.4 346.13
## [1] P2
## Levels: P1 P2 P3
## Start:  AIC=184.24
## `%Hrmax` ~ Av.spd + MechWmin + vL.min + fL.min + V1.min + V2.min
## 
##            Df Sum of Sq    RSS    AIC
## - vL.min    1     0.117 713.98 182.25
## - V1.min    1    12.123 725.99 183.52
## &amp;lt;none&amp;gt;                  713.86 184.24
## - V2.min    1    22.142 736.01 184.56
## - fL.min    1    59.293 773.16 188.30
## - MechWmin  1    77.136 791.00 190.03
## - Av.spd    1    85.748 799.61 190.86
## 
## Step:  AIC=182.25
## `%Hrmax` ~ Av.spd + MechWmin + fL.min + V1.min + V2.min
## 
##            Df Sum of Sq    RSS    AIC
## &amp;lt;none&amp;gt;                  713.98 182.25
## - V2.min    1    22.135 736.12 182.57
## + vL.min    1     0.117 713.86 184.24
## - V1.min    1    43.894 757.87 184.78
## - fL.min    1    61.008 774.99 186.48
## - MechWmin  1    77.841 791.82 188.11
## - Av.spd    1   262.974 976.95 204.08
## [1] P1
## Levels: P1 P2 P3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# not optimal but I run a for loop on the different list to remove the NULL list. It would be faster to use vectorised functions but I need to investigate more on this part. 

keep_list &amp;lt;- c()

for (j in 1:length(listeRes2)){
  if(is.numeric(nrow(listeRes2[[j]]))==T){
    keep_list &amp;lt;- c(keep_list, listeRes2[j])
  }
}

# Using purrr::reduce allow to apply the function bind_rows to all the list in keep_list and is faster than for loops 
res &amp;lt;- purrr::reduce(keep_list, bind_rows)

# We change the long format we have to a wide format to have 1 column for each variable.
Models &amp;lt;- res %&amp;gt;%
  spread(&amp;quot;variable&amp;quot;, value = &amp;quot;value&amp;quot;)

# Replace NA to 0 in our table.
Models[is.na(Models)] &amp;lt;- 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-hr-pred-calculations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Back to HR pred calculations&lt;/h2&gt;
&lt;p&gt;Now, we have done the more complicated part of the problem. We will only use familiar dplyr-based code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Master_HRPred &amp;lt;- Master %&amp;gt;%
 # We filter again only Po &amp;amp; GS drills &amp;amp; dropp all session where we didn&amp;#39;t use HR monitoring.
  dplyr::filter(`Drill Category` == &amp;quot;Po&amp;quot; | `Drill Category` == &amp;quot;GS&amp;quot;) %&amp;gt;%
  dplyr::filter(!is.na(`%Hrmax`)) %&amp;gt;%
# We left join the Models table containing all our models coefficient by Surname to get, for each player, the respective coefficient in the row.
  left_join(Models, by = &amp;quot;Surname&amp;quot;) %&amp;gt;%
# We create a new variable to do the maths - Intercept + ax + by + ...
# In this sample, my 2 models dropped some variables that I need to remove also in my formula. If you were running this code on a full squad, you will need to add all the variables here.  
  mutate(HRpred = `(Intercept)` + (Av.spd.x * Av.spd.y) + (fL.min.x * fL.min.y)  + (V1.min.x * V1.min.y) + (V2.min.x * V2.min.y) + (MechWmin.y * MechWmin.x)) %&amp;gt;%
# HR delta is the difference between real &amp;amp; pred - the lower the better for HRdelta.
  mutate(HRdelta = `%Hrmax`- HRpred) %&amp;gt;%
# Finally grouping &amp;amp; calculating a daily HRpred.
  group_by(Surname, Date) %&amp;gt;%
  summarise(HRdelta = mean(HRdelta, na.rm = T))

write_csv(Master_HRPred, &amp;quot;Master_HRpred.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now 1 row per player &amp;amp; per day that could be easily visualized in R or Tableau. A simple visualisation of this dataset is available on my &lt;a href=&#34;https://public.tableau.com/views/HR-predictions/Tableaudebord1?:embed=y&amp;amp;:display_count=yes&#34;&gt;tableau public account&lt;/a&gt;. If you saw any way to improve this code, either to get it processed faster or more accurate, send me an email and I would be happy to amend this one!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/2_HRpred/Fig3.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to compare one performance with previous ones</title>
      <link>/post/performance-comparison/</link>
      <pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/performance-comparison/</guid>
      <description>&lt;p&gt;When it comes to providing reports to the coaching staff or players, the first impression is always important. Within 5 seconds, a coach can decide &lt;em&gt;not to&lt;/em&gt; have a look at your report. Within 5 seconds, it is always the design and overall aspect of the report which makes a difference, rarely the underlying analyses. In its brilliant paper &lt;a href=&#34;https://martin-buchheit.net/2017/02/18/want-to-see-my-report-coach/&#34;&gt;Want to see my report, Coach&lt;/a&gt;, Martin Buchheit talk about the importance of effective sports science support and rank the attractiveness of the reports among the most important elements to take into account. As such, data needs to be provided to decision-makers in an user-friendly and engaging format &lt;a href=&#34;http://mathlacome.rbind.io/publication/2018-aspetar-rome2/&#34;&gt;2&lt;/a&gt;. As part of our job, we need to compare players performance with previous performance or with pairs daily. In this post, we will try to move step-by-step from the classic &lt;strong&gt;2 bars side-by-side&lt;/strong&gt; chart to something that provides more insights and which is more engaging.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/1_Comparing/BubbleChart0.jpg&#34; alt=&#34;2 bars side-by-side chart to compare player A with the average performance&#34; width=&#34;30%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: 2 bars side-by-side chart to compare player A with the average performance
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/1_Comparing/BubbleChart01.jpg&#34; alt=&#34;Comparing performance with previous performances - another approach&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Comparing performance with previous performances - another approach
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;importing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importing data&lt;/h2&gt;
&lt;p&gt;When we open our Tableau software, the first thing is to import our dataset. This dataset is simple - 1 column with the player name, one with date and one with a performance-related variable (here, Mechanical Work during the game).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-1.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing data&lt;/h2&gt;
&lt;p&gt;We start by moving the ‘Performance variable’ (Mechanical Work (a.u)) into the Column Shelf. The variable is aggregated as the sum of all the data point for this variable and visualized as a bar chart.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-2.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;My goal is to get a point for each different data point (here, dates). We need to take the &lt;strong&gt;Date&lt;/strong&gt; pill from the Dimension Shelf to &lt;em&gt;Details&lt;/em&gt; in the &lt;em&gt;Mark&lt;/em&gt; Shelf. Right click on the &lt;strong&gt;Date&lt;/strong&gt; which is automatically aggregated by Year and select &lt;em&gt;Exact date&lt;/em&gt; (Date exacte in French!). Tableau has changed your bar chart into multiple points along the x-axis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-4.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To move forward, we need to create a Parameter in tableau to select the date we want to compare with the whole dataset. In the left panel, click on &lt;strong&gt;create parameter&lt;/strong&gt; and set it to Date and allow all values.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-Param.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next step is to build a new variable to say “This is the point I want to highlight”. Right click on the &lt;strong&gt;Date&lt;/strong&gt; pills &amp;gt; create &amp;gt; calculated field.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-5.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then, I simply do an If statement, either with a IF xx THEN xx ELSE xx END statement or, easier, IIF formula.&lt;/p&gt;
&lt;p&gt;If my date is the same than the one selected in my parameter, we return a string “Selected Date” (or whatever you want), if not we return “Others”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-6.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Right click on the parameter (bottom shelf, &lt;strong&gt;Selected_Date&lt;/strong&gt; pill) and click on Show parameter command.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-7.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After selecting a date in the parameter, we need to put the &lt;strong&gt;Selected_Date&lt;/strong&gt; from the &lt;em&gt;Dimension Shelf&lt;/em&gt; into Color and Size in the &lt;em&gt;Mark shelf&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-8a.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can play with the Color and Size Mark to customize this visualization.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-8B.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, I use the Analytic panel and drag the Box-plot (french traduction is “Boite à Moustache”!) into my Viz. A customizable box-plot appear in the Viz. I use it often as a way to provide Median and inter-quartile ranges into my Viz. When your players or coaching staff is educated to it, I find it a very efficient way to rank your performance!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/1_Comparing/BubbleChart-9.jpg&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Playing with the different formating option and marks and you can easily end-up with a similar visualization. In my opinion, it is as simple as doing 2 bar charts (or 1 bar and a point!) and much more efficient and engaging.&lt;/p&gt;
&lt;iframe align = &#34;center&#34; width = &#34;750&#34; height = &#34;500&#34; src=&#34;https://public.tableau.com/profile/mathlacome#!/vizhome/MW-performancecomparison/Tableaudebord1??:showVizHome=no&amp;:embed=true&#34;/&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;1 - &lt;a href=&#34;https://martin-buchheit.net/2017/02/18/want-to-see-my-report-coach/&#34;&gt;Want to see my report, coach? Sport science reporting in the real world - Buchheit M.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2 - &lt;a href=&#34;http://mathlacome.rbind.io/publication/2018-aspetar-rome2/&#34;&gt;Monitoring training status with player-tracking technology - Lacome &amp;amp; al.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Readiness or Between-player normalisation</title>
      <link>/post/readiness/</link>
      <pubDate>Mon, 27 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/readiness/</guid>
      <description>&lt;p&gt;We (Martin Buchheit, Ben M. Simpson &amp;amp; me) recently published &lt;a href=&#34;http://mathlacome.rbind.io/publication/2018-aspetar-rome1/&#34;&gt;a paper&lt;/a&gt; dealing with monitoring training status with GPS. If this paper, we argue that &lt;em&gt;“Since player activity patterns are more heavily influenced by contextual variables (e.g. rules, coaches’ interventions, scoreline, drills used) than players’ current fitness status &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/?term=Interpreting+physical+performance+in+professional+soccer+match-play%3A+should+we+be+more+pragmatic+in+our+approach%3F&#34;&gt;1&lt;/a&gt;, locomotor-related variables (level 1 and 2) may not be the most appropriate to directly monitor players’ training status.”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We proposed that player activity normalisation relative to the mean or median of the team (or a sub-group of player) could be another way to look at the data. Indeed, changes in content for a given day will less likely affect this new variable than overall total distance or high speed running. As soon as you get a sufficient amount of data (let’s said 2 months for one player), insights into players’ fitness and/or early signs of fatigue can be gained while following the trends of these normalised data over several consecutive training days.&lt;/p&gt;
&lt;p&gt;In this post, I will show &lt;strong&gt;how simple is it&lt;/strong&gt; to start building similar model and start grabbing information on players.&lt;/p&gt;
&lt;p&gt;We need to load the good library into R - we only need tidyverse to work around the database and openxlsx to load our .xls file where we store the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)       
library(&amp;quot;readxl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I upload the data into R and visualize the format of my database.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Master &amp;lt;- read_excel(&amp;quot;C:/Users/mlacome/Documents/0_Blog (to keep)/Readiness-blog.xlsx&amp;quot;, sheet=&amp;quot;Data&amp;quot;)
head(Master,10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 7
##    `Pos. Grp` `Drill Category` Date                Code    Id_Player
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;            &amp;lt;dttm&amp;gt;              &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    
##  1 AM         Session          2014-06-30 00:00:00 Session P2       
##  2 AM         WU               2014-06-30 00:00:00 WU      P2       
##  3 AM         WU               2014-06-30 00:00:00 WU      P2       
##  4 AM         Ph               2014-06-30 00:00:00 Ph      P2       
##  5 AM         Po               2014-06-30 00:00:00 Po      P2       
##  6 CD         Session          2014-06-30 00:00:00 Session P3       
##  7 CD         WU               2014-06-30 00:00:00 WU      P3       
##  8 CD         WU               2014-06-30 00:00:00 WU      P3       
##  9 CD         Ph               2014-06-30 00:00:00 Ph      P3       
## 10 CD         Po               2014-06-30 00:00:00 Po      P3       
## # ... with 2 more variables: VMAV_min &amp;lt;dbl&amp;gt;, MechW_min &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can see how my database is done. It is a rectangular base with 1 column for position group, one for the drill category, one for the date, the Player name or Id and my GPS variables - I chose only the distance covered above the maximal aerobic velocity and mechanical work.&lt;/p&gt;
&lt;p&gt;For this first exemple, I will look at normalisation to the team only, so I won’t select my first column. I drop this column with &lt;strong&gt;select(-Var)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For our model, as the number of players is highly variable and generally less than 20 for the majority of the drills, data is normalised against the median value rather than the mean because it better represents the central tendency in a small population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Master_ReadiGroup &amp;lt;- Master %&amp;gt;%
  dplyr::select(-`Pos. Grp`) %&amp;gt;%
  # I&amp;#39;m filtering only Possession and Game simulation - I don&amp;#39;t want Tactical and Physical drills #
  dplyr::filter(`Drill Category` == &amp;quot;Po&amp;quot; | `Drill Category` == &amp;quot;GS&amp;quot;) %&amp;gt;%
  dplyr::group_by(Date, `Drill Category`) %&amp;gt;%
  dplyr::summarize(Med_MAV = median(VMAV_min, na.rm=TRUE),
                   Med_MechW = median(MechW_min, na.rm=TRUE))

head(Master_ReadiGroup,10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 4
## # Groups:   Date [8]
##    Date                `Drill Category` Med_MAV Med_MechW
##    &amp;lt;dttm&amp;gt;              &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 2014-06-30 00:00:00 Po                  7.04     0.858
##  2 2014-07-01 00:00:00 Po                  8.74     0.812
##  3 2014-07-02 00:00:00 Po                  3.82     0.687
##  4 2014-07-03 00:00:00 Po                  9.28     0.838
##  5 2014-07-04 00:00:00 GS                  6.22     0.515
##  6 2014-07-04 00:00:00 Po                 11.6      0.841
##  7 2014-07-05 00:00:00 GS                  5.91     0.804
##  8 2014-07-05 00:00:00 Po                  2.32     0.575
##  9 2014-07-06 00:00:00 GS                  8.04     0.797
## 10 2014-07-07 00:00:00 GS                  3.96     0.459&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, for each drill, each day, we have the median of the group for Distance above MAV &amp;amp; MechW.&lt;br /&gt;
We can do the same operation to get the standard deviation (SD), but I prefer to set the SD of the drills to the average SD for each drills (Possessions (Po) &amp;amp; game simulations (GS). When we have a small number of players or outliers, SD can vary a lot and cause trouble for the normalisation.&lt;/p&gt;
&lt;p&gt;Saying that, we can move onto the normalisation of player’s activity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Master_Readi &amp;lt;- Master %&amp;gt;%
  dplyr::select(-`Pos. Grp`) %&amp;gt;%
  dplyr::filter(`Drill Category` == &amp;quot;Po&amp;quot; | `Drill Category` == &amp;quot;GS&amp;quot;) %&amp;gt;%
  ### I join the Master_ReadiGroup containing Median values w/ a Left Join ###
  left_join(Master_ReadiGroup, by = c(&amp;quot;Drill Category&amp;quot;, &amp;quot;Date&amp;quot;)) %&amp;gt;%
  ### I standardise everything w/ Readi = (Value - median) / pre fixed drill SD #
  dplyr::mutate(Readi_MAV = (VMAV_min - Med_MAV)/ifelse(`Drill Category` == &amp;quot;Po&amp;quot;,3.4, 4.2),
                Readi_MechW = (MechW_min - Med_MechW)/ifelse(`Drill Category` == &amp;quot;Po&amp;quot;,0.22, 0.23))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we have 2 options - (1) with players’ activities normalised to the median of the group, or (2) we can start to track this number over time and look for changes in trends. One issue we could face stopping now is that some players might have more variations (noise) than others due to their positionnal role or playing profile. So how can I compare a drop of 5% in one players with a 7% in another one? That’s were I would recommend to normalised again the data, but this time, based on the player individual mean &amp;amp; SD over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Mean_Readi_Player &amp;lt;- Master_Readi %&amp;gt;%
  dplyr::select(Id_Player, `Drill Category`, Readi_MAV, Readi_MechW) %&amp;gt;%
  # Here we group our data by Player and Drills (Po or Gs)
  dplyr::group_by(Id_Player, `Drill Category`) %&amp;gt;%
  dplyr::summarize(Mean_Readi_MAV = mean(Readi_MAV, na.rm=TRUE),
                   Mean_Readi_MechW = mean(Readi_MechW, na.rm=TRUE),
                   SD_Readi_MAV = sd(Readi_MAV, na.rm=TRUE),
                   SD_Readi_MechW = sd(Readi_MechW, na.rm=TRUE))

# Same operation - left join the mean &amp;amp; SD in the former Master_Readi &amp;amp; standardise.
Readiness_Final &amp;lt;- Master_Readi %&amp;gt;%
# Left join by Id_Player AND Drill category to have the appropriate mean &amp;amp; SD for each player.
  left_join(Mean_Readi_Player, by = c(&amp;quot;Id_Player&amp;quot;, &amp;quot;Drill Category&amp;quot;)) %&amp;gt;%
  dplyr::mutate(Readiness_MAV = (Readi_MAV - Mean_Readi_MAV) / SD_Readi_MAV,
                Readiness_MechW = (Readi_MechW - Mean_Readi_MechW) / SD_Readi_MechW) %&amp;gt;%
# In the end, I only want 1 value by day, so I group_by Date &amp;amp; summarise.
  group_by(Id_Player, Date) %&amp;gt;%
  summarise(Readi_MAV = mean(Readiness_MAV),
            Readi_MechW = mean(Readiness_MechW))

head(Readiness_Final, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 4
## # Groups:   Id_Player [2]
##    Id_Player Date                Readi_MAV Readi_MechW
##    &amp;lt;chr&amp;gt;     &amp;lt;dttm&amp;gt;                  &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 P1        2014-06-30 00:00:00    0.222       1.49  
##  2 P1        2014-07-01 00:00:00    0.664       0.355 
##  3 P1        2014-07-02 00:00:00   -0.393      -0.642 
##  4 P1        2014-07-03 00:00:00   -0.740       0.263 
##  5 P1        2014-07-04 00:00:00   -0.0177      0.204 
##  6 P1        2014-07-05 00:00:00    0.399      -0.153 
##  7 P10       2014-06-30 00:00:00   -0.315       0.672 
##  8 P10       2014-07-01 00:00:00    0.741      -0.286 
##  9 P10       2014-07-02 00:00:00   -0.250       0.331 
## 10 P10       2014-07-03 00:00:00   -0.658       0.0622&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, it was a simple exemple of how to work around your data to get more information about how your players cope with the training. We can go further on with rolling mean and look for trends but I will talk about that another time. In my next post, I will explain a concept &lt;strong&gt;a bit more&lt;/strong&gt; complex where we looked at modelisation &amp;amp; heart rate. Let me know if you have any comments. I know that this code can be optimised and would love to heard if you have any optimised solutions. Feel free to email and I would be happy to amend the code or post your solution online!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>/post/welcome/</link>
      <pubDate>Mon, 27 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/welcome/</guid>
      <description>&lt;div id=&#34;why-am-i-doing-this-blog&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why am I doing this blog?&lt;/h2&gt;
&lt;p&gt;I’m Mathieu Lacome, a sports scientist working in elite football with over 10 years of experience in team-sport. A part of my job is to think about how we can improve the way we are collecting data, what we can do with it and how to report findings to the coaching staff or players.&lt;/p&gt;
Over the last 10 years, data feeds increased exponentially. Today, after every single game, data providers such as Opta or Stats produce a large amount of data. Also, we, as practitioners, are collecting data daily, with training load collected with GPS to gym performance obtained from different tools.
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:process&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/welcome-process.jpg&#34; alt=&#34;From data collection to data visualization&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: From data collection to data visualization
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We then store these data in databases that can be as simple as excel sheet to more complex and powerful databases such as Exasol. Then, I have decided - and I’ll explain my choices later in this post - to move most of my analyses into R and reports into Tableau software. In this blog, I will show what we can do with R and Tableau software in term of analysis or visualization to get better insights on how players cope with the training load or team perform. I will explain (and give the code) of the methods used in some of our published papers.&lt;/p&gt;
&lt;p&gt;In this blog, I will not provide an introduction to computer science, R code or data visualization. There are experts for that and I recommend you to look at the references I provide in each sub-section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-i-use-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why I use R&lt;/h2&gt;
&lt;p&gt;When I started my PhD and was analysing data collected on my players, either tracking or physiological data, I was using excel - as almost all student do when they start. After the 10th phone call or meeting with my PhD supervisor, I was ending with a &lt;strong&gt;Analysis_V9_OK.xlsx&lt;/strong&gt; version of my analyses. 5 month after, after sending the paper and receiving the review, I had to delve into my analyses again. At that moment, if I didn’t carefully write all the steps taken to analyse the raw files, I had trouble to reproduce my first results.&lt;/p&gt;
&lt;p&gt;Reproducible research refers to the idea that in “computational” sciences, the ultimate product is not a published paper but rather the entire environment used to produce the results in the paper (data, software, etc.) &lt;a href=&#34;#1&#34;&gt;1&lt;/a&gt;. While it seems normal the research should be reproducible, how can it be if even me struggle to reproduce my initial results! That is the main reason that forced me some years ago to move most of my analyses into R and try to document it. “R is a programming language and free software environment for statistical computing and graphics that is supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.” &lt;a href=&#34;#2&#34;&gt;2&lt;/a&gt; R allow to process your raw files and document (using RMarkdown for example) the how and why’s of your different actions. It can be used to tidy your data, create your models and build your graph for publication.&lt;/p&gt;
&lt;p&gt;Good ressources to start the learning journey:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r4ds.had.co.nz/&#34;&gt;R for Data Science - Hadley Wickham&lt;/a&gt; : Read the book, do the exemples, and share with the community.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/&#34;&gt;Introduction to Statistical Learning - Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/free-introduction-to-r&#34;&gt;DataCamp courses&lt;/a&gt;: Free (introduction) and then low-cost online course. It has been great so far for me.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.edx.org/course/introduction-r-data-science&#34;&gt;EdX courses&lt;/a&gt;: EdX free online course.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-i-use-tableau-software&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why I use Tableau software&lt;/h2&gt;
&lt;p&gt;Effective communication to increase coach buy-in is now one of the more (if not the most!) important soft skills to develop for sports scientists working in an elite set-up &lt;a href=&#34;#3&#34;&gt;3&lt;/a&gt;. With the rise of data visualisation tools (e.g. Tableau Software®, Microsoft Power BI®, Qlik® to cite a few), we are now able to display data in a more effective and engaging way for coaches. &lt;a href=&#34;#4&#34;&gt;4&lt;/a&gt; Among these tools, I decided to use Tableau software for many reasons.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of printing a 20 pages reports, I prefers to use 1 or 2 dashbords and let the end-users play with the filters &amp;amp; parameters.&lt;/li&gt;
&lt;li&gt;In Tableau, I find the design net and almost all graphics can be done in it (e.g., &lt;a href=&#34;https://public.tableau.com/en-us/s/blog/2017/02/data-visualizations-art&#34;&gt;this has been done in Tableau&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;If you want to learn DataViz or if your data are not confidential, you can start with &lt;a href=&#34;https://public.tableau.com/en-us/s/&#34;&gt;Tableau Public&lt;/a&gt; - it is free !&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Where to start learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.vizwiz.com/&#34;&gt;Andy Krieble blog&lt;/a&gt; and &lt;a href=&#34;http://www.makeovermonday.co.uk/&#34;&gt;MakeOverMonday&lt;/a&gt; initiative.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.tableau.com/learn&#34;&gt;Tableau software website&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Big-Book-Dashboards-Visualizing-Real-World/dp/1119282713&#34;&gt;The Big Book of Dashboards - Wexler, Shaffer, Cotgreave&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/Practical-Tableau-Tutorials-Strategies-Master/dp/1491977310&#34;&gt;Practical Tableau: 100 Tips, Tutorials, and Strategies from a Tableau Zen Master - Ryan Sleeper&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;p&gt;1 - &lt;a href=&#34;http://jelena.ece.cmu.edu/repository/conferences/07_04_Icassp_Kovacevic.pdf&#34;&gt;How to encourage and publish reproducible research - Jelena Kovacevic&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2 - &lt;a href=&#34;https://en.wikipedia.org/wiki/R_(langage)&#34;&gt;Wikipedia - R langage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3 - &lt;a href=&#34;https://martin-buchheit.net/2017/02/18/want-to-see-my-report-coach/&#34;&gt;Want to see my report, coach? Sport science reporting in the real world - Buchheit M.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;4 - &lt;a href=&#34;http://mathlacome.rbind.io/publication/2018-aspetar-rome1/&#34;&gt;Monitoring training status with player-tracking technology - Lacome &amp;amp; al.&lt;/a&gt;&lt;/p&gt;
Photo by &lt;a style=&#34;background-color:black;color:white;text-decoration:none;padding:4px 6px;font-family:-apple-system, BlinkMacSystemFont, &amp;quot;San Francisco&amp;quot;, &amp;quot;Helvetica Neue&amp;quot;, Helvetica, Ubuntu, Roboto, Noto, &amp;quot;Segoe UI&amp;quot;, Arial, sans-serif;font-size:12px;font-weight:bold;line-height:1.2;display:inline-block;border-radius:3px&#34; href=&#34;https://unsplash.com/@samuelzeller?utm_medium=referral&amp;amp;utm_campaign=photographer-credit&amp;amp;utm_content=creditBadge&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34; title=&#34;Download free do whatever you want high-resolution photos from Samuel Zeller&#34;&gt;&lt;span style=&#34;display:inline-block;padding:2px 3px&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; style=&#34;height:12px;width:auto;position:relative;vertical-align:middle;top:-1px;fill:white&#34; viewBox=&#34;0 0 32 32&#34;&gt;
&lt;title&gt;
unsplash-logo
&lt;/title&gt;
&lt;p&gt;&lt;path d=&#34;M20.8 18.1c0 2.7-2.2 4.8-4.8 4.8s-4.8-2.1-4.8-4.8c0-2.7 2.2-4.8 4.8-4.8 2.7.1 4.8 2.2 4.8 4.8zm11.2-7.4v14.9c0 2.3-1.9 4.3-4.3 4.3h-23.4c-2.4 0-4.3-1.9-4.3-4.3v-15c0-2.3 1.9-4.3 4.3-4.3h3.7l.8-2.3c.4-1.1 1.7-2 2.9-2h8.6c1.2 0 2.5.9 2.9 2l.8 2.4h3.7c2.4 0 4.3 1.9 4.3 4.3zm-8.6 7.5c0-4.1-3.3-7.5-7.5-7.5-4.1 0-7.5 3.4-7.5 7.5s3.3 7.5 7.5 7.5c4.2-.1 7.5-3.4 7.5-7.5z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;&lt;span style=&#34;display:inline-block;padding:2px 3px&#34;&gt;Samuel Zeller&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sleep quality and quantity of international rugby sevens players during preseason</title>
      <link>/publication/2018-load-sleep/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/2018-load-sleep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Monitoring training status with player-tracking technology. Still on the road to Rome. Part 2</title>
      <link>/publication/2018-aspetar-rome2/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/2018-aspetar-rome2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Monitoring training status with player-tracking technology. Still on the road to Rome. Part 1</title>
      <link>/publication/2018-aspetar-rome1/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/2018-aspetar-rome1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FFR | Science et Rugby | 2018</title>
      <link>/talk/2018-ffr-sportscience/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/talk/2018-ffr-sportscience/</guid>
      <description>&lt;p&gt;This congres was in French but you download the slides here:  &lt;a href=&#34;https://slides.com/mathieulacome/ffr2018/fullscreen&#34; target=&#34;_blank&#34;&gt;Slides&lt;/a&gt;.
If you prefer to watch it, the full video of the congres is there: &lt;a href=&#34;http://formation.ffr.fr/node/697&#34; target=&#34;_blank&#34;&gt;Video&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#34;//slides.com/mathieulacome/ffr2018/embed&#34; width=&#34;800&#34; height=&#34;583&#34; scrolling=&#34;no&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>PSG | Striving for Excellence | 2018</title>
      <link>/talk/2018-psg-striving/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/talk/2018-psg-striving/</guid>
      <description>&lt;p&gt;You can download the slides here:  &lt;a href=&#34;https://slides.com/mathieulacome/psgsummit2018/fullscreen&#34; target=&#34;_blank&#34;&gt;Slides&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&#34;//slides.com/mathieulacome/psgsummit2018/embed&#34; width=&#34;800&#34; height=&#34;533&#34; scrolling=&#34;no&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Training Periodization Over an Elite Rugby Sevens Season: From Theory to Practice</title>
      <link>/publication/2018-training-periodisation-theoriepractice/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/2018-training-periodisation-theoriepractice/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Injury rate and prevention in elite football: let us first search within our own hearts</title>
      <link>/publication/2018-injury-rate-prevention/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0200</pubDate>
      
      <guid>/publication/2018-injury-rate-prevention/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Monitoring of Post-match Fatigue in Professional Soccer: Welcome to the Real World</title>
      <link>/publication/2018-monitoring-pmf/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0100</pubDate>
      
      <guid>/publication/2018-monitoring-pmf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Monitoring players&#39; readiness using predicted Heart Rate responses to football drills</title>
      <link>/publication/2018-readiness-predhr/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0100</pubDate>
      
      <guid>/publication/2018-readiness-predhr/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
